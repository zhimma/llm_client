# LLM Client - OpenAI Compatible Go SDK

[![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?style=flat&logo=go)](https://go.dev/)
[![License](https://img.shields.io/badge/License-Private-red.svg)](LICENSE)

OpenAI å…¼å®¹çš„ Go SDK,ç”¨äºè°ƒç”¨ç»Ÿä¸€ LLM å¹³å°ã€‚

## ç‰¹æ€§

- âœ… **OpenAI å…¼å®¹** - å®Œå…¨å…¼å®¹ OpenAI API è§„èŒƒ
- âœ… **ç±»å‹å®‰å…¨** - å®Œæ•´çš„ Go ç±»å‹å®šä¹‰
- âœ… **æµå¼æ”¯æŒ** - æ”¯æŒ SSE æµå¼å“åº”
- âœ… **æ‰©å±•åŠŸèƒ½** - æ”¯æŒæç¤ºè¯ç®¡ç†ã€ç»“æ„åŒ–æå–ç­‰æ‰©å±•åŠŸèƒ½
- âœ… **æ˜“äºä½¿ç”¨** - ç®€æ´çš„ API è®¾è®¡

## å®‰è£…

```bash
go get github.com/zhimma/llm_client@latest
```

## å¿«é€Ÿå¼€å§‹

### åŸºç¡€å¯¹è¯

```go
package main

import (
    "context"
    "fmt"
    "log"

    llmclient "github.com/zhimma/llm_client"
    "github.com/zhimma/llm_client/types"
)

func main() {
    // åˆ›å»ºå®¢æˆ·ç«¯
    client := llmclient.NewClient(&llmclient.Config{
        BaseURL: "http://localhost:8888/v1",
        APIKey:  "sk-your-api-key",
    })

    // å‘é€å¯¹è¯è¯·æ±‚
    resp, err := client.CreateChatCompletion(context.Background(), types.ChatCompletionRequest{
        Model: "qwen-max",
        Messages: []types.Message{
            {Role: "user", Content: "ä½ å¥½,è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±"},
        },
        Temperature: 0.7,
        MaxTokens:   2000,
    })

    if err != nil {
        log.Fatal(err)
    }

    fmt.Println(resp.Choices[0].Message.Content)
}
```

### æµå¼å¯¹è¯

```go
stream, err := client.CreateChatCompletionStream(ctx, types.ChatCompletionRequest{
    Model:  "qwen-max",
    Messages: []types.Message{{Role: "user", Content: "è®²ä¸ªæ•…äº‹"}},
    Stream: true,
})

if err != nil {
    log.Fatal(err)
}
defer stream.Close()

for {
    chunk, err := stream.Recv()
    if err == io.EOF {
        break
    }
    if err != nil {
        log.Fatal(err)
    }

    fmt.Print(chunk.Choices[0].Delta.Content)
}
```

### ç»“æ„åŒ–æå–(æ‰©å±•åŠŸèƒ½)

```go
resp, err := client.CreateChatCompletion(ctx, types.ChatCompletionRequest{
    Model: "qwen-max",
    Metadata: &types.Metadata{
        PromptKey: "medical_report_expert",
        Variables: map[string]interface{}{
            "text": markdownContent,
        },
    },
})
```

## API æ–‡æ¡£

### Client é…ç½®

```go
type Config struct {
    BaseURL string        // LLM å¹³å°åœ°å€,å¦‚ "http://localhost:8888/v1"
    APIKey  string        // API Key
    Timeout time.Duration // è¯·æ±‚è¶…æ—¶æ—¶é—´,é»˜è®¤ 30s
}
```

### ä¸»è¦æ–¹æ³•

#### Chat Completions

```go
// éæµå¼å¯¹è¯
func (c *Client) CreateChatCompletion(ctx context.Context, req types.ChatCompletionRequest) (*types.ChatCompletionResponse, error)

// æµå¼å¯¹è¯
func (c *Client) CreateChatCompletionStream(ctx context.Context, req types.ChatCompletionRequest) (*types.ChatCompletionStream, error)
```

#### Models

```go
// è·å–æ¨¡å‹åˆ—è¡¨
func (c *Client) ListModels(ctx context.Context) (*types.ModelsList, error)

// è·å–å•ä¸ªæ¨¡å‹ä¿¡æ¯
func (c *Client) GetModel(ctx context.Context, modelID string) (*types.Model, error)
```

## æ‰©å±•åŠŸèƒ½

é€šè¿‡ `Metadata` å­—æ®µå¯ä»¥ä½¿ç”¨å¹³å°çš„æ‰©å±•åŠŸèƒ½:

```go
type Metadata struct {
    PromptKey    string                 `json:"prompt_key,omitempty"`     // æç¤ºè¯ç‰ˆæœ¬æ§åˆ¶
    Variables    map[string]interface{} `json:"variables,omitempty"`      // å˜é‡æ›¿æ¢
    UseMemory    bool                   `json:"use_memory,omitempty"`     // å¯ç”¨è®°å¿†
    UserIdentity map[string]string      `json:"user_identity,omitempty"`  // ç”¨æˆ·èº«ä»½
}
```

## ç¤ºä¾‹

æŸ¥çœ‹ `examples/` ç›®å½•è·å–æ›´å¤šç¤ºä¾‹:

- `examples/chat/` - åŸºç¡€å¯¹è¯ç¤ºä¾‹
- `examples/streaming/` - æµå¼å¯¹è¯ç¤ºä¾‹

## OpenAI å…¼å®¹æ€§

| åŠŸèƒ½ | OpenAI API | æ”¯æŒçŠ¶æ€ |
|------|-----------|---------|
| Chat Completions | `POST /v1/chat/completions` | âœ… |
| Streaming | SSE | âœ… |
| Models List | `GET /v1/models` | âœ… |
| Embeddings | `POST /v1/embeddings` | ğŸš§ è®¡åˆ’ä¸­ |

## è®¸å¯è¯

Private - ä»…ä¾›å†…éƒ¨ä½¿ç”¨
