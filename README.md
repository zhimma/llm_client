# LLM Client - OpenAI Compatible Go SDK

[![Go Version](https://img.shields.io/badge/Go-1.21+-00ADD8?style=flat&logo=go)](https://go.dev/)
[![License](https://img.shields.io/badge/License-Private-red.svg)](LICENSE)

å®Œæ•´çš„ OpenAI å…¼å®¹ Go SDK,æä¾›å¼€ç®±å³ç”¨çš„ LLM åŠŸèƒ½ã€‚

## ç‰¹æ€§

- âœ… **OpenAI å…¼å®¹** - å®Œå…¨å…¼å®¹ OpenAI API è§„èŒƒ
- âœ… **å¼€ç®±å³ç”¨** - ä¸€è¡Œä»£ç å®Œæˆé›†æˆ
- âœ… **å®Œæ•´å°è£…** - Client + Service + Handler ä¸‰å±‚æ¶æ„
- âœ… **ç±»å‹å®‰å…¨** - å®Œæ•´çš„ Go ç±»å‹å®šä¹‰
- âœ… **æµå¼æ”¯æŒ** - æ”¯æŒ SSE æµå¼å“åº”
- âœ… **æ‰©å±•åŠŸèƒ½** - æ”¯æŒæç¤ºè¯ç®¡ç†ã€ç»“æ„åŒ–æå–ç­‰
- âœ… **çµæ´»å¯æ§** - å¯é€‰æ‹©ä½¿ç”¨ä»»æ„å±‚çº§

## å¿«é€Ÿå¼€å§‹

### æ–¹å¼ 1: ä¸€é”®é›†æˆ(æœ€ç®€å•)

```go
package main

import (
    "github.com/gin-gonic/gin"
    llmclient "github.com/zhimma/llm_client"
    "github.com/zhimma/llm_client/handler"
)

func main() {
    r := gin.Default()

    // ä¸€è¡Œä»£ç å®Œæˆ LLM åŠŸèƒ½é›†æˆ
    handler.QuickStart(r, &llmclient.Config{
        BaseURL: "http://localhost:8888/v1",
        APIKey:  "sk-your-api-key",
    })

    r.Run(":8080")
}
```

### æ–¹å¼ 2: åˆ†å±‚ä½¿ç”¨(çµæ´»æ§åˆ¶)

```go
package main

import (
    "github.com/gin-gonic/gin"
    llmclient "github.com/zhimma/llm_client"
    "github.com/zhimma/llm_client/service"
    "github.com/zhimma/llm_client/handler"
)

func main() {
    r := gin.Default()

    // 1. åˆ›å»º Client
    client := llmclient.NewClient(&llmclient.Config{
        BaseURL: "http://localhost:8888/v1",
        APIKey:  "sk-your-api-key",
    })

    // 2. åˆ›å»º Service(å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ è‡ªå®šä¹‰é€»è¾‘)
    svc := service.NewService(client)

    // 3. æ³¨å†Œè·¯ç”±
    llmGroup := r.Group("/v1")
    // å¯ä»¥æ·»åŠ ä¸­é—´ä»¶
    llmGroup.Use(yourAuthMiddleware())
    handler.RegisterRoutes(llmGroup, svc)

    r.Run(":8080")
}
```

### æ–¹å¼ 3: ä»…ä½¿ç”¨ Client(æœ€çµæ´»)

```go
package main

import (
    "context"
    "fmt"

    llmclient "github.com/zhimma/llm_client"
    "github.com/zhimma/llm_client/types"
)

func main() {
    client := llmclient.NewClient(&llmclient.Config{
        BaseURL: "http://localhost:8888/v1",
        APIKey:  "sk-your-api-key",
    })

    resp, err := client.CreateChatCompletion(context.Background(), types.ChatCompletionRequest{
        Model: "qwen-max",
        Messages: []types.Message{
            {Role: "user", Content: "ä½ å¥½"},
        },
    })

    fmt.Println(resp.Choices[0].Message.Content)
}
```

## æ¶æ„

```
llm_client/
â”œâ”€â”€ client.go           # HTTP å®¢æˆ·ç«¯(åº•å±‚)
â”œâ”€â”€ service/           # ä¸šåŠ¡é€»è¾‘å±‚
â”‚   â””â”€â”€ service.go
â”œâ”€â”€ handler/           # Gin è·¯ç”±å¤„ç†å™¨
â”‚   â””â”€â”€ handler.go
â”œâ”€â”€ types/             # OpenAI å…¼å®¹ç±»å‹
â”‚   â”œâ”€â”€ chat.go
â”‚   â”œâ”€â”€ models.go
â”‚   â””â”€â”€ common.go
â””â”€â”€ internal/          # å†…éƒ¨å®ç°
    â”œâ”€â”€ http.go
    â””â”€â”€ stream.go
```

## API æ–‡æ¡£

### Client é…ç½®

```go
type Config struct {
    BaseURL string        // LLM å¹³å°åœ°å€
    APIKey  string        // API Key
    Timeout time.Duration // è¯·æ±‚è¶…æ—¶,é»˜è®¤ 30s
}
```

### Service æ¥å£

```go
type LLMService interface {
    Chat(ctx context.Context, req *types.ChatCompletionRequest) (*types.ChatCompletionResponse, *ChatCompletionStream, error)
    ListModels(ctx context.Context) (*types.ModelsList, error)
    GetModel(ctx context.Context, modelID string) (*types.Model, error)
    ListProviders(ctx context.Context) (interface{}, error)
}
```

### Handler è·¯ç”±

```go
// æ³¨å†Œè·¯ç”±
handler.RegisterRoutes(r, svc)

// æä¾›çš„è·¯ç”±:
// POST /chat/completions  - Chat Completions (OpenAI å…¼å®¹)
// POST /chat             - Chat (å…¼å®¹æ—§è·¯å¾„)
// GET  /models           - æ¨¡å‹åˆ—è¡¨
// GET  /models/:model    - å•ä¸ªæ¨¡å‹
// GET  /providers        - æä¾›å•†åˆ—è¡¨
```

## æ‰©å±•åŠŸèƒ½

é€šè¿‡ `Metadata` å­—æ®µä½¿ç”¨å¹³å°æ‰©å±•åŠŸèƒ½:

```go
resp, err := client.CreateChatCompletion(ctx, types.ChatCompletionRequest{
    Model: "qwen-max",
    Metadata: &types.Metadata{
        PromptKey: "medical_report_expert",  // æç¤ºè¯ç‰ˆæœ¬æ§åˆ¶
        Variables: map[string]interface{}{   // å˜é‡æ›¿æ¢
            "text": content,
        },
    },
})
```

## OpenAI å…¼å®¹æ€§

| åŠŸèƒ½ | OpenAI API | æ”¯æŒçŠ¶æ€ |
|------|-----------|---------|
| Chat Completions | `POST /v1/chat/completions` | âœ… |
| Streaming | SSE | âœ… |
| Models List | `GET /v1/models` | âœ… |
| Embeddings | `POST /v1/embeddings` | ğŸš§ è®¡åˆ’ä¸­ |

## ç¤ºä¾‹

æŸ¥çœ‹ `examples/` ç›®å½•è·å–æ›´å¤šç¤ºä¾‹:

- `examples/chat/` - åŸºç¡€å¯¹è¯
- `examples/streaming/` - æµå¼å¯¹è¯
- `examples/quickstart/` - ä¸€é”®é›†æˆ

## è®¸å¯è¯

Private - ä»…ä¾›å†…éƒ¨ä½¿ç”¨
